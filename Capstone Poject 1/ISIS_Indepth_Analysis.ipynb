{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import fasttext\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the csv file\n",
    "\n",
    "df_tweets = pd.read_csv(\"C:\\\\Users\\\\USER\\\\Documents\\\\Geetha\\\\Data Science\\\\Github\\\\Springboard\\\\Capstone Poject 1\\\\tweet_preprocessed.csv\", parse_dates=True,na_values=' ',encoding=\"ISO-8859-1\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['bismillah', 'acc', 'number', 'follow', 'jzk']\",\n",
       " \"['assalamu', 'alaikum', 'follow']\",\n",
       " \"['ummlina85', 'sent', 'last', 'suspended', 'already']\",\n",
       " \"['rashidunwaleed', 'damn', 'probably', 'follow', 'list', 'fault', 'block']\",\n",
       " \"['ottomanakh', 'erdogan', 'kafir']\",\n",
       " \"['ottomanakh', 'cool']\",\n",
       " \"['ihwco', 'cool', 'blur', 'name', 'random', 'ppl', 'loockin', 'telegram', 'thx']\",\n",
       " \"['ihwco', 'exactly', 'hiding', 'user', 'bros', 'didn', 'screen', 'shot', 'though']\",\n",
       " \"['ihwco', 'thx']\",\n",
       " \"['bismillah', 'suspension']\",\n",
       " \"['guess', 'doing']\",\n",
       " \"['salam', 'alaykum', 'again', 'f0ll0w', 'retweet', 'post', 'jazak', 'khayr', '5upp0rt']\",\n",
       " \"['false']\",\n",
       " \"['bismillah', 'rahman', 'raheem', 'jumada', 'akhirah', '1437', 'hijri']\",\n",
       " \"['islam', 'caliphate', 'declared', 'sunday', '1st', 'ramadan', '1435', '29th', 'june', '2014', '643', 'passed', 'remain']\",\n",
       " \"['suspend', 'account', 'tos', 'violation', 'thx']\",\n",
       " \"['snitches']\",\n",
       " \"['horrifying', 'peacekeepers', 'allegedly', 'tied', 'girls', 'forced']\",\n",
       " \"['account', 'doing', 'harassment', 'delete', 'permanently', 'thank']\",\n",
       " \"['safety', 'suspend', 'account', 'tos', 'violation']\",\n",
       " \"['delete', 'account', 'reason', 'tweeting', 'personal', 'information', 'tos', 'violation']\",\n",
       " \"['delete', 'account', 'permanently', 'doing', 'harassment', 'thx']\",\n",
       " \"['account', 'doing', 'harassment', 'suspend', 'thanks']\",\n",
       " \"['account', 'promoting', 'hate', 'suspend', 'already', 'cmon']\",\n",
       " \"['account', 'doing', 'harassment', 'suspend', 'ask', '10000', 'times', 'now']\",\n",
       " \"['account', 'doing', 'tos', 'violation', 'suspend', 'thx']\",\n",
       " \"['account', 'alamrikiya', '001', 'pretending', 'someone', 'else', 'suspend', 'thank']\",\n",
       " \"['account', 'violating', 'twitter', 'rules', 'suspend', 'thanks']\",\n",
       " \"['account', 'threatening', 'safety', 'suspend']\",\n",
       " \"['account', 'publishing', 'personal', 'information', 'suspend']\",\n",
       " \"['hurry']\",\n",
       " \"['suspend']\",\n",
       " \"['safety', 'suspend']\",\n",
       " '[]',\n",
       " \"['cmon', 'already']\",\n",
       " '[]',\n",
       " \"['yall', 'snitches', 'off', 'top', 'son', 'leave', 'alone', 'cuz', 'just', 'doing', 'own', 'thing', 'get', 'outta', 'her']\",\n",
       " \"['guess', 'learn', 'hard', 'way']\",\n",
       " \"['treats', 'twitting', 'never', 'serious']\",\n",
       " \"['anyways', 'change', 'subjects', 'little', 'bit', 'give', 'opinion']\",\n",
       " \"['style', 'looks']\",\n",
       " \"['breaking', 'explosion', 'paris', 'place', 'evacuated']\",\n",
       " \"['wanna', 'apologize', 'series', 'tweets', 'had', 'too']\",\n",
       " \"['islam', 'fascist', 'false', 'lie']\",\n",
       " \"['prove', 'lies']\",\n",
       " \"['license', 'gun', 'send', 'prove', 'violating', 'rules']\",\n",
       " \"['baqubah']\",\n",
       " \"['ukrainian', 'christian', 'calling', 'islam', 'state']\",\n",
       " '[]',\n",
       " \"['looks', 'surveillance', 'drone']\",\n",
       " '[]',\n",
       " \"['results']\",\n",
       " \"['palestinian', 'authority', 'brink', 'collapse']\",\n",
       " \"['bakr', 'iraq']\",\n",
       " \"['telegram', 'bro', 'sent', 'some', 'scary', 'pics', 'karen', 'spy', 'she', 'looked', 'possessed', 'she', 'looked', 'high', 'laughing']\",\n",
       " \"['iraq', 'forces', 'tried', 'advance', 'according']\",\n",
       " \"['amaq']\",\n",
       " \"['april', 'fools', 'joke', 'coconuts']\",\n",
       " \"['assalam', 'alaikum']\",\n",
       " \"['aamaq', 'diwan', 'zakat', 'distributes', '48m', 'syp', '254', '000', 'among', 'needy', 'raqqa', 'countryside']\",\n",
       " \"['couple', 'released', 'video', 'zakat', 'charitable', 'activities', 'wilayat']\",\n",
       " \"['islam', 'zakat', 'office', 'delivers', 'food', 'baskets', 'financial', 'aid', 'poor', 'families', 'remote', 'areas']\",\n",
       " \"['give', 'power', 'land', 'establish', 'worship', 'give', 'charity', 'enjoin', 'kindness', 'quran']\",\n",
       " \"['wonder', 'arab', 'civilians', 'flee', 'terrorists']\",\n",
       " \"['page', 'naba', 'journal', 'dedicated', 'honoring', 'revolutionary', 'figure', 'media', 'activist', 'soldier', 'bilal', 'homsi']\",\n",
       " \"['nytimes', 'article', 'iconic', 'hero', 'journey', 'member', 'syria', 'revolution', 'council', 'itishhadi']\",\n",
       " \"['maq', 'lebanese', 'soldier', 'wounded', 'attempt', 'advance', 'mountains', 'ras', 'baalbek']\",\n",
       " \"['maq', 'launches', 'tree', 'planting', 'campaign', 'western', 'countryside']\",\n",
       " \"['macqal']\",\n",
       " \"['affiliated', 'maq', 'results', 'led', 'bombings', 'mosul', 'video', 'shows', 'several', 'dead', 'children']\",\n",
       " \"['omashkavash16', 'scum']\",\n",
       " \"['horrible', 'video']\",\n",
       " \"['times', 'senior', 'leader', 'shishani', 'whom']\",\n",
       " \"['army', 'sustained', 'loses', 'soldiers', 'officers', 'big', 'attempt', 'ever']\",\n",
       " \"['martyrdom', 'operation', 'ihsan', 'almani', 'germany', 'qatada', 'shami', 'road', 'haditha', 'baiji', 'killing']\",\n",
       " \"['pics', 'showing', 'destruction', 'army', 'vehicles', 'attempt', 'advance']\",\n",
       " \"['video', 'showing', 'major', 'damages', 'casualties', 'bombing']\",\n",
       " \"['amaq', 'american', 'aircraft', 'destroys', 'buildings', 'downtown', 'link', 'url']\",\n",
       " \"['big', 'blow', 'ring', 'leader', 'financed', 'bakr', 'baghdadi', 'organize', 'carry']\",\n",
       " \"['saudi', 'forces', 'never', 'opened', 'fire', 'jews', 'christians', 'history', 'muslim', 'youths', 'hail']\",\n",
       " \"['saudi', 'govt', 'claimed', 'daeshites', 'apostate', 'cousin', 'badr', 'rashidi', 'saudi', 'officer']\",\n",
       " \"['jazrawi', 'sinai']\",\n",
       " \"['jazrawi', 'sinai', 'yrs', 'old', 'saudi', 'troops', 'preparing', 'bingo', 'daesh', 'home']\",\n",
       " \"['fear', 'war', 'understand', 'everything', 'america', 'taken', 'used', 'against']\",\n",
       " \"['sin', 'sin', 'sin', 'best', 'among', 'sinners', 'repent', 'forgive', 'sins', 'aameen']\",\n",
       " \"['excerpts', 'madinah', 'seerah', 'prophet', 'anwar', 'awlaki', 'share']\",\n",
       " \"['weak', 'plots', 'shaytan']\",\n",
       " \"['irresistible']\",\n",
       " \"['angels', 'greeting', 'jannah']\",\n",
       " \"['poor', 'amongst', 'emigrants', 'amongst', 'enter', 'jannah']\",\n",
       " \"['entering', 'jannah', 'without', 'reckoning']\",\n",
       " \"['invitations', 'haraira']\",\n",
       " \"['rasulullah', 'appoint', 'bilal', 'muadhin', 'conquering', 'makkah']\",\n",
       " \"['abdullah', 'ibn', 'mubarak', 'got', 'such', 'high', 'status']\",\n",
       " \"['oppressed', 'makkah']\",\n",
       " \"['cowards', 'arrogant']\",\n",
       " \"['enters', 'jannah', 'prophets']\",\n",
       " \"['person', 'enter', 'jannah', 'muhammad']\",\n",
       " \"['punishment', 'commits', 'suicide']\",\n",
       " \"['contrary', 'popular', 'belief', 'blacklist', 'rasulullah', 'conquered', 'makkah']\",\n",
       " \"['rasulullah', 'forgives', 'makkah', 'conquering']\",\n",
       " \"['ali', 'asks', 'rasulullah', 'keys', 'bah']\",\n",
       " \"['men', 'born', 'hereafter']\",\n",
       " \"['great', 'reward']\",\n",
       " \"['cutting', 'down', 'sidr']\",\n",
       " \"['seeking', 'religious', 'knowledge', 'dunya']\",\n",
       " \"['cruelty', 'towards', 'animals']\",\n",
       " \"['rasulullah', 'puts', 'customs', 'jahiliyya', 'beneath', 'feet']\",\n",
       " \"['rasulullah', 'had', 'strength', 'conquering', 'makkah']\",\n",
       " \"['umar', 'cleans', 'bah', 'shirk']\",\n",
       " \"['khushoo', 'urwah', 'ibn', 'zubayr']\",\n",
       " \"['short', 'enjoyment', 'life']\",\n",
       " \"['ultimate', 'victory']\",\n",
       " \"['invited', 'through', 'gates', 'jannah']\",\n",
       " \"['invitations', 'gate', 'jannah']\",\n",
       " \"['khalid', 'ibn', 'waleed', 'unintentionally', 'kills', 'muslim']\",\n",
       " \"['rasulullah', 'forgives', 'man', 'tried', 'assassinate']\",\n",
       " \"['ansar', 'feared', 'rasulullah', 'move', 'makkah']\",\n",
       " \"['eyes', 'touch', 'hellfire']\",\n",
       " \"['islam']\",\n",
       " \"['islam', 'status', 'non', 'muslim', 'islam', 'state']\",\n",
       " \"['200', 'believe', 'endure', 'outdo', 'endurance', 'ready', 'observe', 'duty', 'order']\",\n",
       " \"['labels', 'given', 'righteous', 'prophet', 'musa', 'wanted']\",\n",
       " \"['recall', 'oum', 'obeyda', 'never', 'hurted', 'fly', 'yet', 'she', 'custody', '2004', 'injustice', 'oppression']\",\n",
       " \"['mercy2mankind', 'humility', 'rasulullah']\",\n",
       " \"['french', 'kuffar', 'tied', 'young', 'girls', 'forced', 'intercourse', 'dog', 'kill']\",\n",
       " \"['listening', 'quran', 'sound', 'pouring', 'rain', 'perfect', 'beautiful']\",\n",
       " \"['scene', 'makes', 'cry', 'girl', 'crying', 'holding', 'her', 'dead', 'siblings', 'humanity', 'world']\",\n",
       " \"['ask', 'azzawajal', 'grant', 'aseers', 'sweetness', 'iman', 'sakina', 'cells', 'happier', 'whoare', 'free', 'ameen']\",\n",
       " \"['forbidden', 'wear', 'rings', 'seal', 'prophet']\",\n",
       " '[]',\n",
       " \"['islam', 'nobody', 'permission', 'disclose', 'sins', 'anybody', 'confess', 'gesture', 'repe']\",\n",
       " \"['rasulullah', 'fire', 'khalid', 'ibn', 'walid']\",\n",
       " \"['rasulullah', 'merciful', 'man', 'amongst']\",\n",
       " \"['ibn', 'hazm', 'advise', 'someone', 'condition', 'accept', 'oppressor', 'akhl']\",\n",
       " \"['saying', 'imam', 'shaafi', 'turned', 'nasheed', 'laugh']\",\n",
       " \"['according', 'islam', 'state', 'repelled', 'helicopter', 'borne', 'assaults', 'mosul', 'last']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['knows', 'secrets', 'private', 'conversations', 'allah', 'knower']\",\n",
       " \"['remember', 'everything', 'happens', 'reason', 'best', 'planners', 'trust', 'sabr']\",\n",
       " \"['through', 'quran', 'teach', 'lot', 'duas', 'gem', 'amongst', 'quran', '286']\",\n",
       " \"['sahabah', 'path', 'pass', 'deen', 'onto', 'determination']\",\n",
       " \"['bombs', 'civilians', 'vegetable', 'market', 'maarat', 'numan', 'rebels', 'had', 'truce', 'assad']\",\n",
       " \"['rebels', 'jaysh', 'islam', 'had', 'tactical', 'alliance', 'helped', 'forces', 'overcome', 'qalamun']\",\n",
       " \"['dearest', 'sis', 'realized', 'late', 'iam', 'late', 'always', 'muslim']\",\n",
       " \"['preferred', 'strive', 'sit', 'home']\",\n",
       " \"['excerpts', 'shaykh', 'ahmad', 'jibril', 'yaqeen', 'massacre', 'sisters']\",\n",
       " \"['circumstance', 'migration', 'sake']\",\n",
       " \"['conquest', 'makkah']\",\n",
       " \"['life', 'messed']\",\n",
       " \"['complaining', 'doubts', 'shaytaan']\",\n",
       " \"['hatred', 'prevent']\",\n",
       " \"['outside', 'look', 'sinners', 'truly', 'worship', 'darkness', 'night']\",\n",
       " \"['envy', 'rasoolulah', 'saw', 'muslim']\",\n",
       " \"['beautiful', 'kid', 'reacting', 'music', 'quran', 'video', 'become', 'eyeopener']\",\n",
       " \"['every', 'give', 'something', 'sake', 'keep', 'replacing', 'something']\",\n",
       " \"['upon', 'believers', 'rely']\",\n",
       " \"['nor', 'soul', 'die', 'except', 'leave', 'term', 'fixed', 'writing', 'surah', 'imran', '145']\",\n",
       " \"['never', 'give', 'doing', 'good', 'deed', 'reward', 'could', 'difference', 'jannah', 'jahannam', 'expect', 'reward']\",\n",
       " \"['repentance', 'isn', 'apology', 'turning', 'allah', 'turning', 'back', 'ways', 'shaytan']\",\n",
       " \"['bombing', 'innocent', 'muslim', 'protect', 'muslim', 'syria', 'bless', 'ameen']\",\n",
       " \"['decides', 'protect', 'someone', 'nothing', 'harm', 'always', 'seek', 'protection', 'allah']\",\n",
       " \"['sufficient', 'best', 'disposer', 'affairs', 'surat', 'imran', '173']\",\n",
       " \"['earth', 'spacious', 'enough']\",\n",
       " \"['ones', 'gloat', 'believers', 'harmed']\",\n",
       " \"['year', 'old', 'sahabi', 'leading', 'men', 'prayer']\",\n",
       " \"['search', 'truth']\",\n",
       " \"['power', 'quran']\",\n",
       " \"['true', 'imaan']\",\n",
       " '[]',\n",
       " \"['expect', 'guidance', 'matter', 'qiyamul', 'layl', 'beg', 'last', '3rd', 'night']\",\n",
       " \"['diagnosing', 'problem']\",\n",
       " \"['stop', 'looking', 'based', 'benefit', 'start', 'considering', 'how', 'much', 'ben']\",\n",
       " \"['thing', 'questioned']\",\n",
       " \"['promise', 'shaytan']\",\n",
       " \"['broken', 'ladder', 'democracy']\",\n",
       " \"['imagine', 'stopped', 'before', 'going', 'jannah', 'wronged', 'muslim']\",\n",
       " \"['believer', 'balances', 'fear', 'hope']\",\n",
       " \"['miracle', 'rasulullah']\",\n",
       " \"['infant', 'rescued', 'bombardments', 'regime', 'civilian', 'buildings']\",\n",
       " \"['rasulullah', 'orders', 'tree', 'cut', 'down']\",\n",
       " \"['use', 'catapult', 'islam', 'rasulullah']\",\n",
       " \"['wise']\",\n",
       " \"['wisdom', 'delaying']\",\n",
       " \"['horrific', 'aftermath', 'footage', 'regime', 'strikes', 'rebel', 'held', 'sakhour', 'neighbourhood']\",\n",
       " \"['ibn', 'taymiyyah', 'asking', 'allaah', 'forgiveness', 'important']\",\n",
       " '[]',\n",
       " \"['flag', 'ilaha', 'illallah', 'fluttering', 'lands', 'victory', 'living', 'dieing', 'pure', 'tawheed']\",\n",
       " \"['whoever', 'contradicts', 'opposes', 'rasulullah']\",\n",
       " \"['ummah', 'never', 'gets', 'defeated', 'enemies']\",\n",
       " \"['qatadah', 'ghanima']\",\n",
       " \"['rasulullah', 'threw', 'handful', 'dirt', 'defeat', 'kuffar', 'hunain']\",\n",
       " \"['100', 'muslim', 'stood', 'against', '000', 'men']\",\n",
       " \"['turn']\",\n",
       " \"['dont', 'weak', 'pursuit', 'enemy']\",\n",
       " \"['rasulullah', 'injured', 'uhud']\",\n",
       " \"['angels', 'descended', 'hunain']\",\n",
       " \"['rasulullah', 'calls', 'muslim', 'battle']\",\n",
       " \"['muslim', 'caught', 'surprise', 'start', 'flee']\",\n",
       " \"['salah', 'comfort']\",\n",
       " \"['reward', 'surely', 'incumbent', 'upon']\",\n",
       " \"['ratio', 'yajuj', 'majuj', 'hellfire']\",\n",
       " \"['words', 'miqdad']\",\n",
       " \"['married', 'lots', 'children']\",\n",
       " \"['every', 'muslim', 'protecting', 'islam', 'direction']\",\n",
       " \"['intelligence']\",\n",
       " \"['awrah', 'sahabi']\",\n",
       " \"['salah']\",\n",
       " \"['beauty', 'sujood', 'such', 'whisper', 'silently', 'ground', 'heard', 'heavens']\",\n",
       " \"['revolutionary', 'supporter', 'asks', 'viewers', 'muslim', 'world', 'problems', 'gets', 'unexpected', 'answer']\",\n",
       " \"['imagine', 'off', 'reward', 'efforts']\",\n",
       " \"['india', 'jaluthakugai', '000', 'vure', 'gina', 'muslim', 'eba', 'thibi']\",\n",
       " \"['call', 'doing', 'retweets', 'call', 'fitnah']\",\n",
       " \"['something', 'ponder', 'worth', 'reading', 'applying', 'lives']\",\n",
       " \"['names', 'sah']\",\n",
       " \"['names', 'sah']\",\n",
       " \"['names', 'sah']\",\n",
       " \"['drinking', 'water', 'inshaallah', 'post', 'dua', 'everyday', 'follow']\",\n",
       " \"['complain', 'sorrows', 'troubles', 'recompense', 'every', 'pain', 'every', 'loss']\",\n",
       " \"['best', 'river', 'jannah']\",\n",
       " \"['rivers', 'milk', 'honey', 'wine', 'jannah']\",\n",
       " \"['beloved', 'shaykh', 'haqq', 'ahmad', 'jibril', 'hafidhahull']\",\n",
       " \"['misinterpret', 'concept', 'striving', 'feesabilillah']\",\n",
       " \"['true', 'believers', 'look', 'excuses']\",\n",
       " \"['hypocrites']\",\n",
       " \"['fitan', 'few', 'truth']\",\n",
       " \"['hypocrites', 'hurry', 'befriend', 'kuffar', 'fearing', 'misfortune']\",\n",
       " \"['pappii5', 'hihihi', 'son', 'whore', 'stop', 'dreaming', 'home', 'kiss', 'government', 'ass', 'drunk']\",\n",
       " \"['pappii5', 'doesn', 'understand', 'drunken', 'fellow', 'drunk', 'sleep']\",\n",
       " \"['pappii5', 'okay', 'hihi', 'drunken', 'asswhole']\",\n",
       " \"['pappii5', 'tell', 'ziyaad', 'lost', 'wife', 'coz', 'behavior', 'stay', 'away', 'else', 'things', 'cool']\",\n",
       " \"['pappii5', 'hihihi', 'goodbye', 'drunken', 'ziyad']\",\n",
       " \"['pappii5']\",\n",
       " \"['pappii5']\",\n",
       " \"['next', 'generation', 'child', 'fighters', 'threaten', 'world', 'nasheed', 'french']\",\n",
       " \"['pappii5']\",\n",
       " \"['pappii5']\",\n",
       " \"['middleeastnewz', 'akhy']\",\n",
       " \"['pappii5', 'slave', 'god']\",\n",
       " \"['pappii5']\",\n",
       " \"['verily', 'prayer', 'enjoined', 'believers', 'fixed', 'hours', 'qur', '103']\",\n",
       " \"['keep', 'firm', 'obedience', 'loves', 'pleased', 'ameen']\",\n",
       " \"['donate', 'good', 'cause', 'muslim', 'purchasing', 'charity', 'fundraising', 'stall', 'held', 'saturday']\",\n",
       " \"['habits', 'hypocrites']\",\n",
       " \"['muslim', 'spends', 'something', 'family', 'intending', 'receive', 'reward', 'regarded', 'sadaqa', 'bukhari']\",\n",
       " \"['show', 'mercy', 'young', 'ones', 'respect', 'our', 'old', 'ones', 'ahmad']\",\n",
       " \"['everyone', 'shaytaan', 'assigned', 'dunya']\",\n",
       " \"['socializing', 'jannah']\",\n",
       " \"['social', 'life', 'jannah']\",\n",
       " \"['gone', 'forth', 'fallen', 'fitna', 'fallen', 'into', 'fitna', 'staying', 'behind']\",\n",
       " \"['head', 'hypocrites']\",\n",
       " \"['pappii5', 'worship', 'anyone', 'except', 'good', 'parents', 'relatives', 'orphans', 'needy', 'speak', 'kindly']\",\n",
       " \"['attach', 'ourselves', 'enemies', 'expect', 'victory']\",\n",
       " \"['idlib', 'dhin', 'vaige', 'hamalaa', 'meehun', 'maruvejje']\",\n",
       " \"['rylay8']\",\n",
       " \"['opening', 'mosul', 'ghaneema', 'came', 'amazing', 'subhan']\",\n",
       " '[]',\n",
       " \"['jealous']\",\n",
       " \"['must', 'incredible', 'wallah', 'imagine', 'freed', 'prison', 'realise', 'freed', 'khilafa']\",\n",
       " \"['step', 'enforcing', 'islam', 'laws', 'rights', 'protect']\",\n",
       " \"['subhan']\",\n",
       " \"['islam', 'state', 'complete', 'aspects', 'deen', 'alhamdulilah', 'prophetic', 'methodology']\",\n",
       " \"['patients', 'medical', 'center', 'raqah', 'treatment', 'kidney', 'problems']\",\n",
       " \"['directed', 'prophet', 'pbuh', 'children', 'learn', 'pray', 'young', 'age', 'masha']\",\n",
       " \"['usama', 'maghrebi', 'accept', 'beautiful', 'recitation', 'surat', 'tawba']\",\n",
       " \"['look', 'medical', 'lab', 'probably', 'used', 'treat', 'citizens', 'knights', 'democracy', 'bombing', 'everyday']\",\n",
       " \"['borders', 'sykes', 'picot', 'broken', 'alhamdullilah', 'able', 'move', 'freely', 'without', 'previous', 'tyranny']\",\n",
       " \"['bomb', 'night', 'world', 'silent', 'massacre', 'looks', 'civilian']\",\n",
       " \"['mujahid', 'middle', 'battle', 'salam', 'alaikum', 'moderates', 'pass', 'each', 'market', 'trying', 'avoid', 'eye', 'contact']\",\n",
       " \"['leaders', 'lead', 'front', 'politicians', 'spew', 'meaningless', 'words', 'act', 'upon', 'speech']\",\n",
       " '[]',\n",
       " \"['sheikh', 'kerry', 'might', 'sound', 'convincing', 'makes', 'tekfir', 'daqlah', 'evidence']\",\n",
       " \"['morning', 'everything', 'normal', 'until', 'sudden', 'planes', 'started', 'drop', 'bombes', 'houses']\",\n",
       " \"['citizens', 'around', 'media', 'points', 'watch', 'gain', 'knowledge', 'toys', 'too']\",\n",
       " '[]',\n",
       " \"['went', 'reported', 'islam', 'state', 'police', 'thus', 'began', 'investigation']\",\n",
       " \"['fight', 'single', 'sake', 'land', 'resources']\",\n",
       " \"['pappii5']\",\n",
       " \"['bath', 'miswaq', 'surah', 'alkahf', 'increase', 'put', 'perfume', 'wear', 'dress', 'reach', 'mosque']\",\n",
       " '[]',\n",
       " \"['rpg', 'misses', 'istishhadi', 'accept', 'truck', 'fractions', 'allowing', 'him', 'reach', 'target', 'subhan', 'allah']\",\n",
       " \"['ribaat', 'entrance', 'furat', 'ensure', 'safety', 'citizens', 'prevent']\",\n",
       " \"['dar', 'kufr', 'place', 'weak', 'broken', 'believers', 'oppressed', 'strength', 'emaan', 'gets', 'weakened']\",\n",
       " \"['mujahid', 'jund', 'aqsa', 'lost', 'parts', 'jaw', 'battle', 'refuses', 'miss', 'salaah']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['ponder', 'verses', 'qur', 'jews']\",\n",
       " \"['jews', 'hurrying', 'every', 'kind', 'evil']\",\n",
       " \"['protecting', 'friend', 'faith', 'depths', 'darkness', 'lead', 'forth', 'light', '257']\",\n",
       " \"['texanna', 'kick', 'israel', 'presidents', 'butt', 'pig', 'haahaha']\",\n",
       " \"['texanna', 'guys', 'nothing', 'garbage', 'kuffars', 'pakaas']\",\n",
       " \"['texanna', 'gonna', 'kick', 'butt', 'soon']\",\n",
       " \"['sassysassyred', 'pigs', 'fucked', 'recently']\",\n",
       " \"['texanna', 'bunch', 'cowards', 'bring', 'wife', 'slaves', 'kill', 'men', 'soon']\",\n",
       " \"['texanna', 'hitler', 'gave', 'regards', 'jew', 'pig']\",\n",
       " \"['texanna', 'guys', 'look', 'big', 'joke', 'hahaha']\",\n",
       " \"['texanna', 'bring', 'hitler', 'forever', 'jews', 'soon']\",\n",
       " \"['texanna', 'poor', 'pigs', 'fucked']\",\n",
       " \"['scotsmaninfidel', 'childish', 'remarks', 'ass', 'hihihi']\",\n",
       " \"['scotsmaninfidel']\",\n",
       " \"['scotsmaninfidel', 'dumb', 'fuck', 'looser']\",\n",
       " \"['texanna', 'yeah', 'threat', 'pig']\",\n",
       " \"['hitler', 'radical', 'christian', 'admitting', 'christians', 'hell']\",\n",
       " \"['scotsmaninfidel', 'pig']\",\n",
       " \"['texanna', 'jews', 'pigs', 'coming', 'soon', 'bwahahaha']\",\n",
       " \"['texanna', 'guys', 'sleep', 'peace', 'until', 'repent']\",\n",
       " \"['lewolfczu', 'thanks', 'information']\",\n",
       " \"['ele7vn', 'ready', 'judgment']\",\n",
       " \"['lewolfczu']\",\n",
       " \"['pussy', 'boy', 'tryna']\",\n",
       " \"['man', 'tried', 'organize', 'small', 'army', 'gets', 'prison']\",\n",
       " \"['martyrdom', 'operation', 'explosive', 'belt', 'hits', 'members', 'parliament', 'nigerian', 'maiduguri', 'kil']\",\n",
       " \"['kurdish', 'units', 'islam', 'infiltrate', 'shuyukh', 'tahtani', 'kobani']\",\n",
       " \"['sassysassyred', 'bring', 'guys', 'hehe', 'pigs']\",\n",
       " \"['kafirkaty', 'jews', 'pigs']\",\n",
       " \"['sassysassyred', 'jews', 'fucks', 'pigs', 'family', 'pakaad']\",\n",
       " \"['scotsmaninfidel', 'meet', 'field', 'lion', 'see', 'pig']\",\n",
       " \"['spicylatte123', 'educated', 'pigs', 'hehehe']\",\n",
       " \"['kafirkaty', 'nothing', 'huh', 'editing', 'pics']\",\n",
       " \"['spicylatte123', 'laugh', 'tommorrow', 'laugh', 'pakaas']\",\n",
       " \"['spicylatte123']\",\n",
       " \"['scotsmaninfidel', 'guys', 'sleep', 'peace', 'kill', 'each']\",\n",
       " \"['texanna', 'big', 'deal']\",\n",
       " \"['kafirkaty', 'sleep', 'tonight', 'wake', 'tommorrow', 'hell']\",\n",
       " \"['words', 'respond', 'start', 'using', 'pics', 'typical']\",\n",
       " \"['spicylatte123', 'hehe', 'tell', 'story', 'mama']\",\n",
       " \"['kafirkaty', 'fuck', 'coward']\",\n",
       " \"['kafirkaty', 'meet', 'dreams', 'tonight']\",\n",
       " \"['spicylatte123', 'comedy', 'pig']\",\n",
       " \"['scotsmaninfidel', 'keep', 'delusions', 'papa']\",\n",
       " \"['spicylatte123', 'education', 'butt', 'pig']\",\n",
       " \"['kafirkaty', 'rott', 'hell', 'bastard']\",\n",
       " \"['kafirkaty', 'mad', 'pig']\",\n",
       " \"['scotsmaninfidel', 'tell', 'story']\",\n",
       " \"['texanna', 'priceless', 'pig']\",\n",
       " \"['kafirkaty', 'peace', 'tummy']\",\n",
       " \"['scotsmaninfidel', 'mouth', 'rules', 'anything', 'fucker']\",\n",
       " \"['spicylatte123', 'sleep', 'education']\",\n",
       " \"['kafirkaty']\",\n",
       " \"['kafirkaty', 'haha', 'crying', 'face', 'pig']\",\n",
       " \"['spicylatte123', 'hehe', 'afraid']\",\n",
       " \"['kafirkaty', 'sweet', 'dreams', 'reality', 'cry', 'baby']\",\n",
       " \"['kafirkaty']\",\n",
       " \"['spicylatte123']\",\n",
       " \"['kafirkaty', 'wish', 'reality', 'silly', 'pig']\",\n",
       " \"['texanna', 'hahaha']\",\n",
       " \"['kafirkaty', 'show', 'silly', 'pig', 'ready']\",\n",
       " \"['spicylatte123', 'words']\",\n",
       " \"['kafirkaty', 'sound', 'crying', 'little', 'pig']\",\n",
       " \"['kafirkaty']\",\n",
       " \"['kafirkaty', 'maybe', 'noodle']\",\n",
       " \"['spicylatte123', 'indeed', 'called', 'lack', 'ignorant']\",\n",
       " \"['kafirkaty', 'aww', 'silly', 'little', 'pig', 'crying', 'again']\",\n",
       " \"['spicylatte123', 'words', 'hehe']\",\n",
       " \"['kafirkaty', 'shall', 'pigs']\",\n",
       " \"['hitler', 'devout', 'christi']\",\n",
       " \"['kafirkaty', 'show', 'soon', 'bud']\",\n",
       " \"['scotsmaninfidel', 'pigs']\",\n",
       " \"['kafirkaty', 'undoubtful']\",\n",
       " \"['kafirkaty', 'words', 'bruh']\",\n",
       " \"['kafirkaty', 'buddy', 'kick', 'ass', 'cut', 'your', 'head']\",\n",
       " \"['kafirkaty', 'run', 'away', 'dude']\",\n",
       " \"['kafirkaty', 'inbred', 'pig', 'ready', 'run']\",\n",
       " \"['kafirkaty', 'silly', 'little', 'pig', 'ready', 'run']\",\n",
       " \"['kafirkaty', 'yeah', 'cut', 'buddy', 'heat', 'too']\",\n",
       " \"['kafirkaty', 'really', 'pig']\",\n",
       " \"['piccolodaimaojr', 'rat', 'put', 'feet', 'hehe']\",\n",
       " \"['texanna', 'show', 'dad']\",\n",
       " \"['kafirkaty', 'cry', 'shame', 'pig']\",\n",
       " \"['kafirkaty', 'bunch', 'cowards', 'pigs', 'blah', 'blah']\",\n",
       " \"['mark', 'infidel']\",\n",
       " \"['piccolodaimaojr', 'keep', 'dreaming', 'meet', 'soon']\",\n",
       " \"['scotsmaninfidel', 'guys', 'gtg', 'cya', 'around']\",\n",
       " \"['mark', 'infidel', 'dreams']\",\n",
       " \"['khaleefah', 'umar', 'bin', 'abdul', 'aziz', 'stage', 'rule', 'could', 'find', 'single', 'poor', 'person', 'give', 'zakat']\",\n",
       " \"['600', 'iraq', 'civilian', 'against', 'british', 'soldiers', 'rejected', 'court']\",\n",
       " \"['verily', 'angels', 'confer', 'blessings', 'prophet', 'believe', 'confer', 'blessings', 'surah', 'azaab']\",\n",
       " \"['ever', 'noticed', 'reading', 'qur', 'word', 'wrong', 'tongue', 'doesn', 'carry']\",\n",
       " \"['suicide', 'bombers', 'target', 'simultaneous']\",\n",
       " '[]',\n",
       " \"['muslim', 'isn', 'always', 'easy', 'gives', 'strength']\",\n",
       " \"['scotsmaninfidel', 'talking']\",\n",
       " \"['marcosjr', 'avila', 'talking', 'bible']\",\n",
       " \"['fighters', 'retake']\",\n",
       " \"['sin', 'wouldn', 'attractive', 'punishment', 'sin', 'paid', 'immediately', 'gives', 'repent', 'turn']\",\n",
       " \"['friday', 'forgiving', 'love', 'forgiveness', 'forgive']\",\n",
       " \"['evil', 'akhirah', 'man', 'consorts', 'wife', 'publicizes', 'her', 'secr']\",\n",
       " \"['prophet', 'fever', 'heat', 'hell', 'fire', 'cool', 'water', 'bukhari']\",\n",
       " \"['aala']\",\n",
       " \"['baqiya101', 'aslamalikum', 'akhi', 'give', 'shout', 'jazkallahkhair']\",\n",
       " \"['kebab', 'ayran', 'asslamalikum', 'akhi']\",\n",
       " \"['muhammad', 'kuno', 'supporters', 'pledged', 'allegiance']\",\n",
       " \"['ismedia', 'hang', 'kaffirs']\",\n",
       " \"['write2', 'dear', 'friend', 'mohammad', 'salah', 'uddin', 'a3862dn', 'hmp', 'belmarsh', 'london', 'se28', '0eb']\",\n",
       " \"['skxx82x', 'shout', 'bro', 'sis', 'jzk']\",\n",
       " \"['protect']\",\n",
       " \"['amirbakistani47', 'bulk', 'anonymity', 'disturb', 'whole']\",\n",
       " \"['able', 'understand', 'language', 'feel', 'pain', 'bless']\",\n",
       " \"['extremely', 'fierce', 'army', 'followed', 'multiple', 'vbieds']\",\n",
       " \"['ihatealsaud', 'kufr', 'mind', 'understand', 'struggle', 'mujahid', 'donkey', 'good', 'mind', 'kufr']\",\n",
       " \"['absialbaghdadi', 'mujahid', 'welcome', 'kufr', 'soon']\",\n",
       " \"['broke', 'heart', 'criminal', 'doing', 'muslim', 'children']\",\n",
       " \"['babies', 'born', 'die', 'dec']\",\n",
       " \"['allahu', 'akbar', 'hilafet', 'askerleri', 'bat', 'kalemunda', 'nusayri', 'rejiminin', 'sava', 'aksavarlarla']\",\n",
       " \"['salam', 'alaykum', 'sisters', 'islam', 'shotout', 'protect', 'all', 'sisters', 'jazakallah', 'khair']\",\n",
       " \"['moussa', 'arrested', 'incarcerated', 'providing', 'aid', 'rohingya', 'muslim', 'bangladesh']\",\n",
       " \"['breaking', 'car', 'bomb', 'airport', 'interior', 'ministry', 'casualties', 'feared', 'officials']\",\n",
       " \"['kufr', 'french', 'die', 'rage']\",\n",
       " \"['war']\",\n",
       " \"['nearly', 'half', 'million', 'takes', 'action', 'against']\",\n",
       " \"['accounts', 'release', 'story', 'khalid', 'britani', 'died', 'martyrdom', 'operation', 'october']\",\n",
       " \"['hit', 'civilians', 'russia', 'pilots', 'well', 'trained', 'never', 'missed', 'targets']\",\n",
       " \"['forget', 'sisters', 'dont', 'tired', 'speaking']\",\n",
       " \"['elwalaawalbaraa', 'subhanallah', 'kufr', 'media', 'spreading', 'false']\",\n",
       " '[]',\n",
       " \"['000', 'town', 'dying', 'flies', 'regime', 'hunger', 'siege']\",\n",
       " \"['children', 'besieged', 'starving', 'town', 'enjoy', 'nice', 'meal', 'leaves', 'yes', 'leaves']\",\n",
       " '[]',\n",
       " \"['special', 'units', 'heightened', 'alert', 'tehran', 'streets', 'threats', 'islam', 'state']\",\n",
       " \"['2015', 'asylum', 'seekers', 'europe', 'children', 'met', 'greece']\",\n",
       " \"['civilians', 'starving', 'death', 'besieged', 'iran', 'hizballah', 'civilian', 'dead']\",\n",
       " \"['women', 'islam', 'shake', 'cradle', 'baby', 'hand', 'shake', 'throne', 'kufr']\",\n",
       " \"['bintemergent', 'love', 'horse', 'riding']\",\n",
       " \"['allahu', 'akbar', '000', 'times']\",\n",
       " \"['every', 'path', 'short', 'cut', 'shortcut', 'path', 'jannah', 'jihad', 'fisabililah', 'hasan', 'basri', 'sure']\",\n",
       " '[]',\n",
       " \"['destroy', 'mocking', 'starving', 'children', 'rabb', 'teach', 'lesson']\",\n",
       " \"['hello', 'world', 'palestinian', 'baby', 'bet', 'never', 'heard', 'media', 'blocks', 'reaching']\",\n",
       " \"['palestinian', 'girl', 'dead', 'shot', 'israeli', 'heart']\",\n",
       " '[]',\n",
       " \"['french', 'teenager', 'jewish', 'teacher', 'machete', 'islam', 'state']\",\n",
       " \"['protect', 'muslim']\",\n",
       " \"['old', 'fighting', 'islam', 'state', 'hard', 'believing', 'seduce', 'young']\",\n",
       " \"['europe', 'faces', 'style', 'year', 'top', 'expert', 'warns']\",\n",
       " \"['kebab', 'ayran', 'asslamalikum', 'akhi', 'surprise', 'following', 'follow', 'akhi']\",\n",
       " \"['kebab', 'ayran', 'chatsecure', 'working', 'tried']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['maq', 'fighters', 'foreign', 'nationals', 'forces', 'protecting']\",\n",
       " \"['jakarta', 'indonesia', 'affiliated']\",\n",
       " \"['photo', 'province', 'teenager', 'saying', 'farewell', 'conducting', 'martyrdom', 'operation']\",\n",
       " \"['muslim', 'amongst', 'romans']\",\n",
       " \"['wake', 'muslim', 'next']\",\n",
       " \"['asslamalikum', 'akhi', 'shout', 'jzakallah', 'khair']\",\n",
       " \"['never', 'forgive', 'never', 'forget', 'never', 'question', 'rose', 'against', 'tyrant']\",\n",
       " \"['call', 'yourself', 'modest', 'hijaabi', 'plaster', 'face', 'social', 'media', 'modest']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['war', 'islam', 'used', 'lie']\",\n",
       " \"['umm', 'ummah', 'nation', 'preserve', 'women', 'preserve', 'nation', 'akbar']\",\n",
       " \"['david', 'cameron', 'government', 'once', 'again', 'using', 'muslim', 'political', 'football', 'score', 'cheap', 'points']\",\n",
       " \"['rewarding', 'way', 'speak', 'swt']\",\n",
       " \"['live', 'terrorist', 'bacha', 'khan', 'university', '3000', 'students', 'trapped']\",\n",
       " \"['love', 'care', 'allah', 'loves', 'much', 'such', 'honour', 'subhanall']\",\n",
       " \"['comes', 'provision']\",\n",
       " \"['love', 'love', 'muhammad', 'love', 'qur', 'love', 'islam']\",\n",
       " \"['forgotten', 'neglected', 'goes', 'bakr', 'baghdadi']\",\n",
       " \"['young', 'idlib', 'stay', 'night', 'thanks']\",\n",
       " \"['kebab', 'ayran']\",\n",
       " \"['warplanes', 'wiped', 'entire', 'homes', 'town', 'hayyan']\",\n",
       " \"['asraa', 'sisters', 'sendet', 'through', 'kids', 'draw', 'forget']\",\n",
       " \"['subhanallah', 'nerve', 'complain', 'smallest', 'things', 'yet', 'biggest', 'blessings', 'granted']\",\n",
       " \"['forgive', 'god', 'haven', 'thanked', 'eyes']\",\n",
       " \"['advice', 'quran']\",\n",
       " \"['blesses', 'financially', 'raise', 'standard', 'living', 'raise', 'standard', 'giving']\",\n",
       " \"['greater', 'trials', 'storms', 'depression', 'doubts', 'worries', 'fears', 'addictions', 'greater']\",\n",
       " \"['displaced', 'children', 'baghdad', 'look', 'winter', 'supplies', 'benefit', 'families']\",\n",
       " \"['muslim', 'muslim', 'muslim', 'die', 'insha', 'muslim', 'proud', 'muslim']\",\n",
       " \"['asked', 'paths', 'paradise', 'path', 'shorter', 'jihad']\",\n",
       " \"['deed', 'closer', 'exalted', 'majestic', 'dutifulness', 'ibn', 'abbas']\",\n",
       " \"['maq', '544', 'regime', 'soldiers', 'month', 'january']\",\n",
       " \"['believed', 'fear', 'grant', 'criterion', 'remove', 'misdeeds']\",\n",
       " \"['qur', 'impact', 'heart', 'mind', 'got', 'read', 'sincerity']\",\n",
       " \"['sent', 'qur', 'down', 'verses', 'clear', 'evidence', 'guides', 'whom', 'intends']\",\n",
       " \"['muslim', 'world', 'retweet', 'ameen']\",\n",
       " \"['every', 'son', 'adam', 'commits', 'sins', 'best', 'commit', 'sin', 'repent']\",\n",
       " \"['remember', 'write', 'letters', 'tweets']\",\n",
       " \"['urgent', 'countless', 'fleeing', 'hoping', 'escape', 'regime']\",\n",
       " \"['footage', 'releases', 'video', 'airstrike', 'communications', 'facility']\",\n",
       " \"['earth', 'mosque', 'wherever', 'prayer', 'pray', 'sahih', 'muslim']\",\n",
       " \"['lot', 'account', 'suspended', 'every', 'login', 'account', 'confirmation', 'code', 'asking', 'leave']\",\n",
       " \"['corruption', '205']\",\n",
       " \"['committed', 'atrocities', 'fight', 'could', 'responsible', 'deaths']\",\n",
       " \"['izzadeen', 'transferred', 'belmarsh', 'omar', 'trevor', 'brooks', 'a0085aq', 'hmp', 'bullingdon', 'bicester', 'ox25']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['twin', 'homemade', 'grenade']\",\n",
       " \"['unborn', 'baby', 'umbilical', 'cord', 'attached', 'mom', 'both', 'russia', 'fuck']\",\n",
       " \"['despair', 'never', 'lose', 'hope', 'mercy']\",\n",
       " \"['israeli', 'soldiers', 'shot', 'old', 'palestinian', 'girl', 'left', 'her', 'bleed', 'death', 'inna', 'lillahi', 'inna', 'ilaihi', 'rajioon']\",\n",
       " \"['amaq', 'app']\",\n",
       " '[]',\n",
       " \"['islam']\",\n",
       " '[]',\n",
       " \"['moschea']\",\n",
       " '[]',\n",
       " \"['information', 'dodged', 'image', 'map', 'shows', 'strongholds', 'delivering', 'incorrect', 'information']\",\n",
       " \"['breaking', 'gun', 'education', 'dept', 'saudi', 'arabia', 'gunman', 'arrested']\",\n",
       " \"['saudi', 'arabia', 'offered', 'broaden', 'participation', 'led', 'strikes', 'against']\",\n",
       " \"['wrap', 'important', 'stories']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['abdusmujahid149']\",\n",
       " \"['three', 'soldiers', 'pkk', 'sur']\",\n",
       " \"['staggering', 'price', 'reconstruction']\",\n",
       " \"['russia', 'dmitry', 'everyone', 'must', 'negotiate', 'instead', 'unleashing', 'world', 'war']\",\n",
       " \"['alhayatmediacenter', 'ten', 'selected', 'videos', 'wilayat', '1st', 'jumada', 'ula', '1437']\",\n",
       " \"['depth', 'saudi', 'arabia', 'confirmed', 'plans', 'send', 'troops']\",\n",
       " \"['shoffner', 'estimated', '000', '000', 'islam', 'state', 'fighters', 'part', 'nangarhar']\",\n",
       " \"['update', 'russia', 'comments', 'world', 'war', 'appear', 'mistranslated', 'reuters', 'article']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['infographic', 'foreign', 'involvement']\",\n",
       " \"['army', 'reinforcements', 'arriving', 'situation', 'clear', 'yet']\",\n",
       " \"['usa', 'backed', 'usa', 'backed', 'usa', 'led', 'forces', 'soon', 'too']\",\n",
       " \"['living', 'life', 'until', 'war', 'criminals', 'bombed', 'house', 'goodbye', 'world', 'basma', 'humsi']\",\n",
       " \"['video', 'turkish', 'forces', 'celebrating', 'kurdish', 'majority', 'cizre', 'declaring', 'success']\",\n",
       " \"['ahudhayfah', 'inspiration']\",\n",
       " \"['video', 'iraq', 'drone', 'strikes', 'against']\",\n",
       " \"['obama', 'abe', 'park', 'meet', 'washington', 'discuss', 'pressure', 'acc']\",\n",
       " \"['kurds', 'allies', 'seize', 'base', 'backed', 'russia', 'monitor']\",\n",
       " \"['lavrov', 'part', 'ceasefire', 'forgot', 'interpretation']\",\n",
       " \"['islam', 'schools']\",\n",
       " \"['russia', 'peace', 'envoys']\",\n",
       " \"['repelling', 'failed', 'attempt', 'nusayri', 'army', 'raid', 'western', 'airport']\",\n",
       " \"['russia', 'naemeh', 'aftermath', 'footage', 'putin', 'crimes']\",\n",
       " \"['russia', 'ghanto', 'far']\",\n",
       " \"['members', 'stripped', 'rank', 'membership', 'damaging', 'civilian', 'housing']\",\n",
       " \"['government', 'admitted', 'use', 'smart', 'home', 'devices', 'spying', 'trevor', 'timm']\",\n",
       " \"['america', 'losing', 'battle', 'url']\",\n",
       " \"['photo', 'pilot', 'successfully', 'ejects', 'plane', 'goes', 'down']\",\n",
       " \"['infographic', 'suicide', 'january', '2016']\",\n",
       " '[]',\n",
       " \"['happening', 'every', 'militants', 'tanks', 'gone', 'army', 'approached']\",\n",
       " \"['turkey', 'saudi', 'arabia', 'could', 'soon', 'invade', 'yea', 'right']\",\n",
       " \"['bnt', 'mhz', 'jazakkallah', 'khair']\",\n",
       " \"['ozwitness133', 'jazakkallah', 'khair', 'aakhi']\",\n",
       " \"['432mryam', 'jazakkallah', 'khair']\",\n",
       " \"['ak47dawakafiri', 'jazakkallah', 'khair']\",\n",
       " \"['monitor', 'life', 'normal', 'dowla', 'anywhere', 'else', 'iraq']\",\n",
       " \"['kufr', 'bit', 'tagoot', 'jazakkallaah', 'khair', 'aakhi']\",\n",
       " \"['jstmryam', 'jazakkallha', 'khair']\",\n",
       " \"['afran', 'kha', 'jazakkallah', 'khair']\",\n",
       " \"['generalfalluj40', 'jazakkallah', 'khair']\",\n",
       " \"['afriqbin', 'jazakkallah', 'khair', 'aakhi']\",\n",
       " \"['bintpaladin', 'mufti', 'five', 'star', 'murthad', 'calling', 'kufur', 'jahiliyya']\",\n",
       " \"['ukhti']\",\n",
       " \"['generalfalluj40', 'aameen']\",\n",
       " \"['grand', 'sheikh', 'highest', 'jannah', 'hoping', 'meet', 'jannah']\",\n",
       " \"['middleeasteye', 'yes', 'muslim', 'war', 'against', 'murthad', 'crusaders']\",\n",
       " \"['newsleak', 'war', 'islam', 'poor', 'muslim']\",\n",
       " \"['sahabah', 'path', 'pass', 'deen', 'onto', 'determinatio']\",\n",
       " \"['akhi']\",\n",
       " \"['screw', 'tweeta', 'understand', 'expect', 'something', 'kuffar', 'world']\",\n",
       " \"['foreignpolicy', 'islam', 'against', 'democracy', 'islam', 'law', 'democracy', 'man', 'made', 'law']\",\n",
       " \"['bismillah', 'acc', 'number', 'follow', 'jzk']\",\n",
       " \"['bismillah', 'again']\",\n",
       " \"['madkhalies', 'khawarij', 'muslim', 'ummah', 'murjiah', 'rulers']\",\n",
       " \"['michaeldweiss', 'nonsense', 'cowards', 'balls', 'fact']\",\n",
       " \"['michaeldweiss', 'militias', 'care', 'abt', 'civilians', 'always', 'loot', 'torture', 'civilians', 'happy', 'bombing', 'civilians']\",\n",
       " \"['khateebdimashqi', 'fake', 'jihadis', 'order', 'frm', 'pak', 'thagooth', 'good', 'terrorists']\",\n",
       " \"['mistaken', 'kamal', 'hero', 'muslim', 'beginning', 'later', 'realized', 'greatest', 'traitor']\",\n",
       " \"['yea', 'such', 'similirity', 'jowlani', 'kamal']\",\n",
       " \"['bismillahir', 'rahmanir', 'raheem', 'assalamo', 'alaykom', 'told', 'shall', 'return']\",\n",
       " \"['hjr', 'jhd', 'common', 'dowla', 'criminal', 'kuffar', 'hate', 'dowla']\",\n",
       " \"['best', 'generation', 'ever', 'takbirrr']\",\n",
       " '[]',\n",
       " \"['ameen']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['pls', 'share', 'sisters']\",\n",
       " '[]',\n",
       " \"['photo', 'israeli', 'soldier', 'holding', 'palestinian', 'boy', 'headlock', 'goes', 'viral']\",\n",
       " \"['tawheed1', 'important', 'live', 'one']\",\n",
       " \"['word', 'week']\",\n",
       " \"['ironmuhajir2', 'nothing', 'jihad', 'god', 'give', 'power', 'farrida']\",\n",
       " \"['heart', 'tears', 'blood', 'every', 'kids', 'occupied', 'loved', 'country']\",\n",
       " \"['sisters', 'show', 'determination', 'haters', 'never', 'trier', 'banning', 'haleemah', 'never', 'stop', 'sho']\",\n",
       " \"['mainstream', 'media', 'reporting', 'bail', 'conditions']\",\n",
       " \"['thx', 'god', 'lovely', 'shiekh', 'free', 'wish', 'meet', 'soon', 'teacher', 'wish', 'freedom', 'preson']\",\n",
       " \"['muslim', 'eat', 'halal', 'meat']\",\n",
       " \"['pls', 'tweet']\",\n",
       " \"['good', 'shaam', 'corrupt']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['protect', 'bad']\",\n",
       " \"['akhmad', 'john', 'link', 'opening', 'bro', 'way', 'open']\",\n",
       " \"['nabi', 'saw', 'weight', 'seed', 'arrogance', 'heart', 'enter', 'paradise', 'muslim']\",\n",
       " \"['muslim', 'where', 'ared', 'real', 'shame', 'disappointed']\",\n",
       " \"['summary', 'happened', 'masjid', 'aqsa', 'morning', 'israel', 'thinks', 'stop', 'coming', 'pray']\",\n",
       " \"['winter', 'started', 'means', 'cloud', 'less', 'light', 'more', 'depression']\",\n",
       " '[]',\n",
       " \"['knows', 'best', 'way', 'apply', 'shop', 'real', 'islam', 'manor']\",\n",
       " \"['subhan', 'amazing', 'love', 'care', 'sisters', 'though', 'haven', 'met', 'sab']\",\n",
       " \"['ummhussain', 'masha', 'made', 'important', 'step', 'life', 'allah', 'honors']\",\n",
       " \"['iraq', 'houthy', 'militia', 'booms', 'civilian', 'mention']\",\n",
       " \"['yesss', 'proudly']\",\n",
       " \"['tell', 'chiekh', 'speak', 'way', 'sake', 'western', 'govs', 'brain', 'washed', 'media']\",\n",
       " \"['big', 'propaganda']\",\n",
       " \"['electricity', 'water', 'oil', 'work', 'rebuild', 'living', 'prison', 'life', 'miracle', 'gaza']\",\n",
       " \"['islam', 'roadshow', 'lewisham', 'accept', 'efforts', 'conveying', 'message', 'islam', 'ameen']\",\n",
       " \"['ahead', 'lions']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['heart', 'love', 'respect', 'bilad', 'sisters', 'hope', 'lead', 'right', 'way']\",\n",
       " \"['publish', 'statement', 'held', 'towns']\",\n",
       " '[]',\n",
       " \"['interesting', 'statement', 'ummah']\",\n",
       " \"['adjamessaoud', 'ameen']\",\n",
       " \"['putin', 'kremlin', 'leader', 'bomb', 'outside', 'former', 'soviet', 'union', '1979']\",\n",
       " \"['mountakem', 'jabbar', 'rab']\",\n",
       " \"['guidance', 'matter', 'intelligent', 'without', 'allah', 'guidance', 'never', 'succeed']\",\n",
       " \"['read', 'press', 'release', 'published', 'family', 'sheikh', 'hamza', 'regarding', 'situation']\",\n",
       " \"['tawheednetwrk', 'protect', 'murtadin', 'kuffar', 'allah', 'easy', 'him']\",\n",
       " \"['ask', 'protect', 'pur', 'beloved', 'sheikh', 'harm', 'hasbunaallah']\",\n",
       " \"['every', 'muslim', 'needs', 'public', 'react', 'hearing', 'qur']\",\n",
       " \"['sins', 'committed', 'son', 'adam', 'sins', 'tongue']\",\n",
       " \"['wish', 'kids', 'happiness', 'live', 'big', 'lie']\",\n",
       " \"['fatih', 'mubrak', 'akhi', 'rewards', 'wishes', 'keep', 'him', 'tawheed', 'all', 'life']\",\n",
       " '[]',\n",
       " \"['islam', 'reminders', 'walaa', 'causes', 'love', 'slave']\",\n",
       " \"['ra18', 'fatih', 'iyakum', 'remember', 'akika', 'calibration']\",\n",
       " \"['quran', 'companion', 'life', 'quran', 'companion', 'solitude', 'grave']\",\n",
       " \"['find', 'door', 'closed', 'upon', 'never', 'despair']\",\n",
       " \"['snapshot', 'friendship']\",\n",
       " \"['call', 'upon', 'seeking', 'mercy', 'forgiveness']\",\n",
       " '[]',\n",
       " \"['sad', 'muslim', 'killing', 'each', 'enemy', 'watching', 'happily', 'unite', 'our', 'muslim']\",\n",
       " \"['had', 'nothing', 'against', 'except', 'believed', 'all', 'mighty', 'worthy', 'all', 'praise', 'quran85']\",\n",
       " \"['john', 'something', 'bad', 'john', 'blamed', 'ahmed', 'does', 'something', 'bad', 'islam', 'blamed', 'double', 'standards', 'blame']\",\n",
       " \"['cannot', 'school', 'study', 'work', 'afford', 'little', 'needs', 'family']\",\n",
       " \"['ameen']\",\n",
       " \"['swt', 'due', 'apply', 'bail', '2015']\",\n",
       " '[]',\n",
       " \"['sister']\",\n",
       " '[]',\n",
       " \"['nutsflipped', 'pls', 'dont', 'tell']\",\n",
       " \"['trophy']\",\n",
       " \"['before', 'hezbollah', 'iran', 'gangs', 'transformed', 'beautiful']\",\n",
       " \"['sha', 'islam', 'manor', 'love', 'wish']\",\n",
       " \"['children', 'victim', 'refugee', 'camp', 'dec']\",\n",
       " \"['enemies']\",\n",
       " \"['follow']\",\n",
       " \"['follow']\",\n",
       " \"['reynaldo', 'vegas1', 'interesting', 'article', 'jazak']\",\n",
       " \"['sister', 'pls']\",\n",
       " \"['streets', 'roads', 'empty', 'london', 'ghosts', 'shaking', 'lions', 'khilafa', 'making', 'wold', 'safer', 'place']\",\n",
       " \"['takbeeerr']\",\n",
       " \"['allahu', 'akbar']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['qualities', 'islam', 'good', 'mohammed', 'pbuh', 'replied', 'feed', 'poor', 'greet', 'whom', 'whom']\",\n",
       " \"['rebel', 'leader', 'assassinated', 'earlier', 'had', 'praised', 'terrorists', 'paris']\",\n",
       " \"['keep', 'going', 'lions']\",\n",
       " \"['listen']\",\n",
       " \"['jahannem']\",\n",
       " \"['baqiyah']\",\n",
       " \"['graphic', 'look', 'shia', 'militias', 'doing', 'muslim', 'civilians', 'iraq']\",\n",
       " \"['feeling', 'darul', 'kufr', 'read', 'dabiq', 'before']\",\n",
       " \"['expected', 'tonight', 'macedonia', 'serbia', 'border', 'families', 'crossing', 'risk', 'freezing']\",\n",
       " \"['person', 'knows', 'earn', 'tomorrow', 'quran']\",\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " '[]',\n",
       " \"['cluster', 'bombs', 'thrown', 'civilians', 'jets']\",\n",
       " \"['scholars', 'launches', 'orthodox', 'crusaders', 'campaign', 'levant', 'every', 'muslim', 'must', 'fund', 'muja']\",\n",
       " \"['fierce', 'army', 'bombardment']\",\n",
       " \"['sums', 'policies']\",\n",
       " \"['clear', 'footage', 'jet', 'dropping', 'payload', 'somewhere']\",\n",
       " \"['assalam', 'alaykom', 'give', 'shout', 'ikhwa', 'unable', 'recover', 'previous', 'account']\",\n",
       " \"['destroyed', 'concept', 'nationalism', '1400', 'believers', 'single', 'surah', 'hujurat', 'ayah']\",\n",
       " \"['three', 'suicide', 'bombers', 'military', 'qasr', 'hotel']\",\n",
       " \"['saudi', 'council', 'senior', 'scholars', 'calls', 'upon', 'islam', 'nation', 'everything', 'power', 'oppressed', 'mujahideen']\",\n",
       " \"['informs', 'foreign', 'partners', 'ready', 'set', 'contacts', 'aim', 'unite']\",\n",
       " \"['expect', 'reps', 'possibly', 'talks', 'moscow', 'pretty', 'soon', 'set', 'coordination', 'gro']\",\n",
       " \"['ready', 'set', 'contacts', 'russia', 'avoid', 'ready', 'come', 'moscow']\",\n",
       " \"['entire', 'families', 'bombardment', 'bodies', 'found']\",\n",
       " \"['massacre', 'airstrike', 'market', 'hold', 'town', 'bab', '100']\",\n",
       " \"['follow']\",\n",
       " \"['carolinajose19']\",\n",
       " \"['shot', 'down', 'army', 'helicopter', 'pilot', 'reportedly']\",\n",
       " '[]',\n",
       " \"['fleefromdunya', 'retweet', 'abayo', 'jazakaallah', 'khayr']\",\n",
       " '[]',\n",
       " \"['she', 'her', 'son', 'prison', 'march', '2014', 'forget', 'her']\",\n",
       " '[]',\n",
       " \"['salamualikum', 'fam', 'umm', 'miskeen', 'follow', 'shout', 'jazakallah', 'khair']\",\n",
       " \"['country', 'waging', 'war', 'beards']\",\n",
       " '[]',\n",
       " \"['small', 'military', 'veterans', 'throw', 'off', 'medals', 'outside', 'downing', 'street', 'protest', 'bombing']\",\n",
       " \"['nice', 'try', 'justifying', 'war', 'crimes', 'reality', 'target', 'hospitals', 'bridges', 'grain', 'silos', 'purpose']\",\n",
       " \"['fast', 'learners']\",\n",
       " \"['shaykh', 'ahmad', 'prison']\",\n",
       " \"['hassanistiila', 'filthy', 'nationalists']\",\n",
       " \"['important', 'watch', 'ahmed', 'musa', 'jibril', 'message', 'speak', 'without', 'knowledge', 'golden', 'advice']\",\n",
       " \"['ahhijra', 'lot', 'sure', 'aside', 'nothing', 'roundtable', 'global', 'jihad']\",\n",
       " \"['again', 'follow', 'spread', 'jazakallahu', 'khair']\",\n",
       " \"['services', 'department', 'repairs', 'power', 'lines', 'aqueducts', 'opens', 'alternative', 'diversion', 'routes', 'closure', 'bridges']\",\n",
       " \"['cleanup', 'recovery', 'efforts', 'ala', 'bless', 'reward']\",\n",
       " \"['suspended', 'president', 'largest', 'muslim', 'ngo', 'france', 'refusal', 'condemn']\",\n",
       " \"['prophet', 'woman', 'created', 'rib', 'try', 'straighten', 'rib', 'break', 'treat', 'women', 'kindly']\",\n",
       " \"['believers', 'die', 'silent', 'kafir', 'gets', 'injured', 'all', 'begin', 'bark']\",\n",
       " \"['suspension']\",\n",
       " \"['keep', 'habibah', 'iyah', 'along', 'her', 'family', 'barakall', 'feekum']\",\n",
       " \"['ppl', 'sakeofallah', 'sister', 'windsofparadise', 'she', 'coma', 'yest', 'normal', 'morn']\",\n",
       " \"['keep', 'making', 'she', 'got', 'coma']\",\n",
       " \"['guys', 'check', 'emily', 'family', 'had', 'pleased', 'all']\",\n",
       " \"['talk', 'talk', 'doubts', 'hearts', 'neve', 'pull', 'actions', 'coz', 'stain', 'hearts', 'purify', 'yourself']\",\n",
       " \"['state', 'seeing', 'lot', 'dreams', 'lately', 'big', 'fut7at', 'coming', 'soon', 'inshaallah', 'thabaat', 'needed', 'until']\",\n",
       " \"['ikhwa']\",\n",
       " \"['listened', 'almost', 'usulaththalatha', 'explanations', 'both', 'arabic', 'anyone', 'comes', 'close']\",\n",
       " \"['photo', 'arabi', 'nineveh', 'province', 'link']\",\n",
       " \"['photo', 'busy', 'street', 'dawwasah', 'link']\",\n",
       " \"['blessed', 'land', 'caliphate', 'rainy']\",\n",
       " \"['soldiers', 'conducted', 'istishhadi', 'operations', 'against', 'hwiqah', 'rishdiyyah']\",\n",
       " \"['fighter', 'destroys', 't62', 'anti', 'armor', 'guided', 'missile', 'wadi', 'airbase']\",\n",
       " \"['fighter', 'turkish', 'artillery', 'fire', 'caused', 'destruction', 'houses', 'mosques', 'destroy']\",\n",
       " \"['aamaq', 'footage', 'shows', 'newly', 'liberated', 'villages', 'countryside', 'ghazl', 'yani', 'yaban']\",\n",
       " \"['soo', 'true', 'sins', 'shackles', 'through', 'night', 'waking', 'qiyam']\",\n",
       " \"['indicates', 'person', 'weak', 'religion', 'fear', 'god', 'seeks', 'opinion', 'suits', 'desire', 'ibn']\",\n",
       " \"['sure', 'check', 'hashes']\",\n",
       " \"['watch', 'story', 'ata', 'ibn', 'abi', 'rabah', 'sulayman', 'ibn', 'abd', 'malik', 'shaykh', 'ahmad', 'musa', 'jibril', 'youtube']\",\n",
       " '[]',\n",
       " \"['salaf', 'say', 'soon', 'knows', 'death', 'come', 'suddenly', 'self', 'deception', 'regardin']\",\n",
       " \"['bismill', 'asal', 'mualaikum', 'following', 'observing', 'need', 'shoutout']\",\n",
       " \"['handing', 'naba', 'journal']\",\n",
       " \"['man', 'rare', 'tree', 'skin', 'condition', 'receive', 'specialist', 'treatment']\",\n",
       " \"['unconfirmed', 'ieds', 'exploded', 'center']\",\n",
       " \"['breaking', 'ten', 'iraq', 'soldiers', 'detonation', 'explosive', 'device', 'burabi']\",\n",
       " \"['amaq', 'islam', 'state', 'fighters', 'capture', 'number', 'buildings', 'entrenched', 'iraq', 'forces', 'sufiyyah', 'neighborhood']\",\n",
       " \"['alhamdulillahi', 'rabbil', 'alamin']\",\n",
       " \"['timeline', 'without', 'colonel', 'shami', 'end', 'times', 'dreams', 'without', 'sfaxian', 'awiya']\",\n",
       " \"['seconds']\",\n",
       " \"['ummting']\",\n",
       " \"['man', 'cannot', 'love', 'music', 'love', 'quran']\",\n",
       " '[]',\n",
       " \"['fighter', 'shaheed', 'baljiki', 'waving', 'flag', 'water']\",\n",
       " \"['war', 'children', 'smile', 'resilient', 'dictators', 'throw']\",\n",
       " \"['dear', 'sisters', 'care', 'tweet', 'good', 'haqq', 'picture', 'inappropriate', 'retweet']\",\n",
       " \"['ibn', 'taymiyyah', 'having', 'sins', 'correct', 'tawh', 'having', 'less', 'sins', 'corrupt', 'tawh']\",\n",
       " '[]',\n",
       " \"['sha']\",\n",
       " \"['ask', 'sake', 'azza', 'jal']\",\n",
       " \"['inghimasi', 'residential', 'complex', 'belongs', 'ain', 'base', 'baghdadi', 'dead']\",\n",
       " \"['ponder']\",\n",
       " \"['rafidhi', 'army', 'militias', 'car', 'bomb', 'northwest', 'fallujah']\",\n",
       " '[]',\n",
       " \"['missed', 'baqiyah', 'family', 'lot', 'glad']\",\n",
       " \"['salamu', 'alaykum', 'rahmatullahi', 'barakatuhu', 'suspension', 'account']\",\n",
       " \"['beware', 'evil', 'cholar']\",\n",
       " \"['french', 'nasheed', 'released']\",\n",
       " \"['mentioned', 'nasheed', 'featured', 'latest', 'alhayat', 'media', 'release', 'accompanied', 'paris', 'umar']\",\n",
       " \"['war', 'dummies', 'bomb', 'innocent', 'ppl', 'ppl', 'mad', 'join', 'terrorist', 'groups', 'war', 'terror', 'basically', 'means']\",\n",
       " \"['yourself', 'terrorism', 'blowing', 'hospital', 'hellfire', 'missile', 'fighter', 'jet']\",\n",
       " \"['fitnah', 'opposite', 'gender', 'great', 'safe', 'shaykh', 'daee', 'one', 'actually', 'worse']\",\n",
       " \"['centre', 'diatique', 'alhayat', 'sente', 'par', 'amour']\",\n",
       " \"['mufti', 'monk', 'smiles', 'taghut']\",\n",
       " '[]',\n",
       " \"['share', 'account', 'freedom', 'speech', 'miss', 'censored', 'strive', 'god', 'willing', 'humble', 'servan']\",\n",
       " \"['seen', 'adulterers', 'obscene', 'sodomites', 'drug', 'addicts', 'hate', 'bqz', 'hate', 'sharia']\",\n",
       " \"['sugarcoat', 'islam', 'masses', 'contrary', 'lot', 'kuffar', 'drawn', 'due', 'justic']\",\n",
       " \"['else', 'munafiqun', 'plot', 'against', 'supposed', 'sisters']\",\n",
       " \"['shahadah', 'tongue', 'actions', 'prove', 'opposite', 'valid', 'allahu', 'alam']\",\n",
       " \"['risk', 'such', 'aqidah', 'fail', 'moment', 'trust', 'moderate', 'muslim']\",\n",
       " \"['ithnillah', 'saudi', 'ithnil', 'amreeka']\",\n",
       " '[]',\n",
       " \"['close', 'relative', 'khayr', 'sha']\",\n",
       " \"['opinion', 'doesn', 'count', 'comes', 'qur', 'sunnah']\",\n",
       " \"['lot', 'felt', 'sad', 'fall', 'minnigh', 'heretic', 'pkk', 'pyd', 'sacrifices', 'led']\",\n",
       " \"['campaign', 'successful', 'received', 'response', 'ummah']\",\n",
       " \"['amanah', 'upon', 'deliver', 'books', 'imprisoned', 'sisters', 'barakallahu', 'feekom']\",\n",
       " \"['unable', 'perform', 'qiyaam', 'layl', 'fast', 'deprived', 'shackled', 'sins']\",\n",
       " \"['could', 'contemplate', 'words', 'every', 'read', 'reading', 'time']\",\n",
       " \"['muhammad', 'ibn', 'yoosuf', 'sufyaan', 'ath', 'thawri', 'lead', 'qiyaam', 'young', 'men', 'pray']\",\n",
       " \"['qataadah', 'hypocrite', 'never', 'stay', 'night', 'prayer']\",\n",
       " \"['night', 'prayer', 'means', 'protecting', 'oneself', 'fitan', 'trials', 'shaykh', 'ibn', 'uthaymeen']\",\n",
       " \"['give', 'sadaqah', 'without', 'delay', 'stands', 'way', 'calamity', 'tirmidhi', 'text', 'warm93', '70070']\",\n",
       " \"['follow', 'insta', 'gems']\",\n",
       " \"['check', 'chain', 'tweets', 'shaa', 'allaah', 'share', 'family', 'friends', 'intention', 'attaining']\",\n",
       " \"['give', 'priority', 'arabic', 'own', 'language']\",\n",
       " \"['ramiallolah']\",\n",
       " \"['lanaat', 'kuffar', 'amiiiiin']\",\n",
       " \"['celebrating', 'valentine', 'act', 'kufr', 'protect', 'imitating', 'kuffar']\",\n",
       " \"['previous', 'retweets', 'enraged', 'hypocrites']\",\n",
       " \"['banatulummah', '07899000930']\",\n",
       " \"['alhamdhulillaah', 'write', 'whole', 'ayah']\",\n",
       " \"['cherry', 'picking', 'verses', 'qur', 'sugar', 'coat', 'romanticise', 'deen', 'sad']\",\n",
       " \"['admit', 'mistakes', 'write', 'ayah', 'full', '216']\",\n",
       " \"['road', 'tinged', 'rosewater', 'petals']\",\n",
       " \"['tounges', 'remember', 'alot', 'few', 'hearts', 'tag', 'along', 'tongue', 'shaykh', 'mad', 'jib']\",\n",
       " \"['dar', 'islam', 'quran', 'competition', 'organized', 'diwan', 'wah', 'masajid']\",\n",
       " \"['strugglinukht', 'khayr', 'sha']\",\n",
       " \"['sad', 'dear', 'sister', 'sad', 'ask', 'muwahideen', 'dear', 'sister', 'ask', 'reunite', 'her']\",\n",
       " \"['read', 'quran', '600', 'pages', '30days', 'pages', '20pages', 'salat', 'pages', 'per', 'salah', 'pages', 'before']\",\n",
       " \"['word', 'looking', 'fat']\",\n",
       " \"['arabiya', 'daily', 'mail', 'arab', 'world']\",\n",
       " \"['sweet', 'revenge', 'revenge', 'sweet', 'alhamdulillah']\",\n",
       " \"['really', 'hope']\",\n",
       " \"['ahmad', 'musa', 'jibril', 'salaf', 'had', 'list', 'names', 'night', 'prayers']\",\n",
       " \"['last', 'afia', 'siddiqui', 'appeared', 'court', 'she', 'wearing', 'niqab', 'avoid', 'posting', 'her', 'old', 'pics', 'respec']\",\n",
       " \"['sufy', 'ibn', 'differ', 'advise', 'refer', 'muj', 'hid']\",\n",
       " \"['frontier', 'posts', 'waged', 'jih', 'surely', 'guide']\",\n",
       " \"['ankab', 'tafs', 'qurtub']\",\n",
       " \"['shaykhul', 'isl', 'ibn', 'taymiyyah', 'ibn', 'qayyim', 'credited', 'statement', 'ibn', 'mub', 'rak', 'ahmad']\",\n",
       " \"['ibn', 'taymiyyah', 'regarding', 'general', 'living', 'frontiers', 'performing', 'rib']\",\n",
       " \"['concerning', 'oneself', 'rib', 'great', 'matter', 'frontier', 'posts', 'inhabited', 'best', 'muslim']\",\n",
       " \"['best', 'lands', 'establishing', 'rites', 'islam', 'realities', 'commanding', 'good']\",\n",
       " \"['everyone', 'wanted', 'dedicate', 'himself', 'worship', 'devote', 'himself']\",\n",
       " \"['achieve', 'best', 'zuhd', 'worship', 'awareness', 'scholars', 'direct', 'towards', 'frontier', 'posts']\",\n",
       " \"['remember', 'bosnian', 'witness', 'got', 'shahada', 'accept', 'ameen']\",\n",
       " '[]',\n",
       " \"['du3a', 'hamed', 'abderrahaman', 'arrested', 'guantanamo', 'recently', 'arrested', 'spain']\",\n",
       " \"['three', 'muslim', 'american', 'kids', 'ages', 'murdered', 'execution', 'style', 'indiana']\",\n",
       " \"['oumdujana', 'allahuma', 'barik']\",\n",
       " \"['grand', 'jury', 'indictment', 'handed', 'down', 'dear', 'ukhti', 'muslim', 'safya', 'roe', 'yassin', 'held', 'without', 'bail', 'keep']\",\n",
       " \"['bismillah', 'set', 'fundraising', 'page']\",\n",
       " '[]',\n",
       " \"['idh', 'alqarnee', 'got', 'shot', 'philippines', 'punish', 'never', 'forget', 'took', 'part', 'issuing', 'fatawa']\",\n",
       " \"['deal', 'kuffar', 'palace', 'scholars', 'allah', 'avenge', 'muwahideen', 'wrongfully']\",\n",
       " \"['emotional', 'situation', 'forget', 'happened', 'deal', 'unjust']\",\n",
       " \"['hasan', 'basri', 'quote', 'gets', 'every', 'subhanaallah']\",\n",
       " \"['suspect', 'kind', 'information', 'kind', 'cyber', 'operation', 'government', 'undermine', 'internet']\",\n",
       " \"['followed', 'jihad', 'iraq', 'aware', 'crucible', 'islam', 'caliphate', 'accepted']\",\n",
       " \"['avoid', 'friendship', 'constantly', 'inquire', 'discuss', 'flaws', 'arabic', 'proverb']\",\n",
       " \"['salsabeela', 'wats', 'macho']\",\n",
       " \"['salsabeela', 'thank']\",\n",
       " '[]',\n",
       " \"['breaking', 'peshmurga', 'apostates', 'istishhadi', 'assault', 'shandokhah', 'talafar']\",\n",
       " \"['elevate', 'highest', 'level', 'jannah', 'allahu', 'akbar', 'sharia', 'allah', 'front', 'eyes']\",\n",
       " \"['milksheikh2', 'high', 'leave', 'her', 'mentions', 'dont', 'her', 'bit', 'good']\",\n",
       " \"['anyone', 'spreading', 'believing', 'nonsense', 'sheep', 'doesn', 'deception', 'works']\",\n",
       " \"['against', 'islam', 'state', 'ground', 'use', 'sorcerers', 'media']\",\n",
       " \"['always', 'remember', 'pilots', 'kill', 'muslim', 'children', 'considered', 'american', 'national', 'heroes', 'protector', 'fre']\",\n",
       " \"['hillary', 'emails', 'egyptian', 'military', 'had', 'covertly', 'moved', 'early', '2011']\",\n",
       " '[]',\n",
       " \"['britain', 'set', 'deploy', '000', 'troops', 'fight', 'against']\",\n",
       " \"['photos', 'oil', 'installation', 'suffers', 'significant', 'damage', 'following']\",\n",
       " \"['cyber', 'caliphate', 'army', 'announces', 'collective', 'hackers', 'called', 'sons', 'caliphate', 'army']\",\n",
       " \"['trial', 'militants']\",\n",
       " \"['tries', 'jihadists']\",\n",
       " \"['saifeddine', 'raies', 'former', 'spokesperson', 'sharia', 'arrested', 'home', 'kairouan', 'sources']\",\n",
       " \"['backed', 'terrorist', 'ansar', 'sharia', 'media', 'person', 'islam']\",\n",
       " \"['looks', 'famous', 'bmp', 'zpu', 'combo', 'ansar', 'sharia', 'destroyed', 'tika']\",\n",
       " \"['senior', 'ansar', 'sharia', 'aqap', 'leader', 'majid', 'mohamed', 'abdullah', 'murshid', 'houthi', 'forces']\",\n",
       " \"['strange', 'bedfellows', 'washington', 'prefers', 'speak']\",\n",
       " \"['isolated', 'qaeda', 'chief', 'losing', 'recruits', 'funds']\",\n",
       " \"['weekly', 'march', 'march', '2016', 'including']\",\n",
       " \"['captures', 'rebel', 'stronghold', 'saham', 'jawlan', 'map']\",\n",
       " \"['hit', 'sunk', 'thanks', 'allahu', 'akbar']\",\n",
       " \"['answering', 'question']\",\n",
       " \"['next', 'step', 'misrata']\",\n",
       " \"['ever', 'wants', 'good', 'puts', 'test']\",\n",
       " \"['saad', 'ziyad', 'leaders', 'never', 'betrayed']\",\n",
       " \"['destruction', 'hummers', 'belonging', 'iraq', 'forces']\",\n",
       " '[]',\n",
       " \"['decide', 'upon', 'affair', 'associates', 'your', 'affair', 'worry', 'quran', 'yunus']\",\n",
       " \"['leader', 'iraq', 'popular', 'mobilization', 'shiite', 'militants', 'units']\",\n",
       " \"['shabab', 'seized', 'large', 'quantity', 'weapons', 'military', 'trucks', 'adde', 'enough', 'supplies']\",\n",
       " \"['division', 'growing', 'imposed', 'government', 'decision', 'making']\",\n",
       " \"['ummah', 'lost', 'great', 'man', 'lost', 'anwar', 'awlaki', 'taqabbalahu']\",\n",
       " \"['swear', 'disbelievers', 'palestinian', 'cause', 'reject', 'instantly', 'gaza', 'called', 'islam']\",\n",
       " \"['held', 'hands', 'bloody', 'assault']\",\n",
       " \"['mustafaklash34']\",\n",
       " \"['unbelievably', 'gory', 'images', 'coming', 'diyala', 'iraq', 'beheading', 'burning', 'ppl', 'alive', 'hands', 'pro', 'govt', 'militias', 'zero']\",\n",
       " \"['account', '8000', 'tank', 'shells', '500', 'kornet', 'rockets', 'seized', 'radio', 'hill', 'depot']\",\n",
       " \"['little', 'stuff', 'seized', 'radio', 'hill', 'imagine', 'depots']\",\n",
       " \"['didnt', 'understood', 'radicalism']\",\n",
       " \"['stop', 'supporting', 'jihad', 'government', 'anti', 'radicalization', 'program', 'jihadi', 'ever']\",\n",
       " '[]',\n",
       " \"['regained', 'ghazal', 'yani', 'bayan', 'qarah', 'kubri', 'turkish', 'backed', 'rebels']\",\n",
       " \"['france', 'muslim', 'france', 'population', 'jail']\",\n",
       " \"['baraghida', 'bel', 'turkish', 'backed', 'islam', 'rebels']\",\n",
       " \"['fire', 'throwing', 'gas', 'letting', 'burn']\",\n",
       " \"['told', 'cannt', 'unite']\",\n",
       " \"['libya', 'rejects', 'backed', 'peace', 'deal', 'unity', 'government']\",\n",
       " \"['terrifying', 'sound', 'russia', 'warplanes', 'causes', 'pregnant', 'women', 'lose', 'unborn', 'babies', 'deirezzor']\",\n",
       " \"['last', 'army', 'surroundings', 'brigade', '128', 'fell']\",\n",
       " \"['abdullah', 'bin', 'amr', 'come', 'mosques', 'won', 'single']\",\n",
       " \"['ian', 'forces', 'jet', 'crashed', 'unknown', 'reasons', 'both', 'pilots', 'reportedly']\",\n",
       " \"['moses', 'ala', 'split', 'sea', 'obey', 'punch', 'sea', 'stick']\",\n",
       " \"['abraham', 'replace', 'son', 'big', 'sheep', 'obey', 'slaughter', 'son']\",\n",
       " \"['incidents', 'increasing', 'becaming', 'quite', 'notable', 'shot', 'down', 'combat', 'drone']\",\n",
       " \"['isil', 'kill', 'iraq', 'forces']\",\n",
       " \"['refuses', 'western', 'use', 'airspace', 'strikes', 'against']\",\n",
       " \"['country', 'notorious', 'tight', 'grip', 'media', 'controls', 'cannot']\",\n",
       " \"['jaishal', 'islam', 'invited', 'democratic', 'opposition', 'telling', 'form', 'islam', 'state']\",\n",
       " \"['death', 'toll', 'bombing', 'against', 'shia', 'shrine', 'damascus', 'monitor']\",\n",
       " \"['pics', 'media', 'never', 'show', 'caused']\",\n",
       " \"['300', '000', 'deaths', 'shaam', 'kuffar', 'care', 'journalists', 'muslim', 'pray4paris', 'donkeys']\",\n",
       " \"['distance', 'lift', 'siege', 'kweirs', 'nubl', 'zahra', 'approximately', 'needed', 'hours']\",\n",
       " \"['1436']\",\n",
       " \"['breaking', 'islam', 'state', 'takes', 'jusiya', 'border', 'post', 'linking', 'jurud', 'qaa', 'lebanon', 'qusayr', 'countryside']\",\n",
       " \"['aircrafts', 'several', 'raids']\",\n",
       " '[]',\n",
       " \"['telegram', 'channel', 'diary', 'mujahid', 'degenerating', 'something', 'else']\",\n",
       " \"['aircrafts', 'grain', 'stores', 'countryside', '2nd', 'weeks']\",\n",
       " '[]',\n",
       " \"['islam', 'operations', 'bangladesh']\",\n",
       " \"['ibnamaan', 'decides', 'upon', 'something']\",\n",
       " '[]',\n",
       " \"['god', 'gave', 'illness', 'remind', 'number', 'muhammad', 'ali']\",\n",
       " \"['life', 'carrying', 'normal', 'russia', 'daily', 'basis', 'daily', 'raqqa', 'daily', 'fronts']\",\n",
       " \"['second', 'civilian', 'casualties', 'save']\",\n",
       " \"['isl', 'spread', 'things', 'quran', 'sword', 'helps']\",\n",
       " \"['current', 'situation', 'map', 'wilayat', 'kheyr', 'closing']\",\n",
       " \"['follow']\",\n",
       " \"['irony', 'sahwat', 'excuses', 'kufr', 'democracy', 'alliance', 'kuffar', 'yet', 'bash', 'condemn']\",\n",
       " \"['matters', 'guilty', 'sins', 'excuses', 'clear', 'kufr', 'condemn', 'imaginary', 'falsified', 'sins']\",\n",
       " \"['eyes', 'sahwat', 'fictitious', 'sin', 'greater', 'blatant', 'kufr']\",\n",
       " \"['month', 'khilafa', 'distributed', '5million', '722', 'poor', 'families', 'meanwhile', 'saudi', 'waliul', 'amr']\",\n",
       " \"['note', '5million', 'wilayats', '9million', 'just', 'alone', 'despite', 'whole', 'world', 'fighting']\",\n",
       " \"['mustafaklash37', 'loool', 'daleel', 'against', 'assertion', 'must', 'really', 'hurt', 'eccentric', 'ego', 'great', 'sheykh']\",\n",
       " \"['nikore', 'warya']\",\n",
       " \"['loool']\",\n",
       " \"['kazabalanka2', 'loool', 'jabhat', 'alkhusrah', 'okay', 'huh', 'hypocrisy', 'double', 'standards', 'crazy']\",\n",
       " \"['jabhat', 'jolani', 'bragged', 'surprise', 'look', 'fanboy', 'got', 'bashed']\",\n",
       " \"['tweet', 'bragged', 'surprise', 'jamaat', 'baghdadi', 'crying']\",\n",
       " \"['reminds', 'jews', 'quds', 'oppressors', 'criminals', 'yet', 'feign', 'innocence', 'act', 'victims', 'tfuu', 'aleyk', 'yaa', 'sahwat', 'ridda']\",\n",
       " \"['munafiqs', 'sahabas', 'used', 'humiliated', 'ones', 'bold', 'empowered', 'untill', 'sect', 'called', 'salafiyah']\",\n",
       " \"['notice', 'difference', 'preaches', 'tawheed', 'implements', 'sacrifices', 'preaches', 'dresses', 'fed']\",\n",
       " \"['aqeedah', 'filthy', 'madhkali', 'munafiq', 'blaming', 'suffering', 'muslim', 'rebellion', 'against', 'taghut']\",\n",
       " \"['inal', 'munafiqeena', 'fidarkil', 'asfari', 'minanar']\",\n",
       " \"['video', 'message', 'islam', 'state', 'new', 'video', 'message', 'islam', 'state']\",\n",
       " \"['video', 'message', 'islam', 'state', 'had', 'enough', 'falsehood', 'wil', 'yat', 'alab']\",\n",
       " \"['looooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooool', 'going', 'hell']\",\n",
       " \"['read', 'anything', 'read']\",\n",
       " \"['islam', 'state', 'wilaya', 'halab', 'message', 'beloved', 'halab']\",\n",
       " \"['good', 'lecture', 'series', 'constants', 'path', 'jihad', 'anwar', 'awlaki']\",\n",
       " \"['dirtttttt']\",\n",
       " \"['manhaj', 'establishing', 'shariah', 'shaikh', 'faisal']\",\n",
       " \"['prays', 'five', 'times', 'jahannam', 'nayef', 'sahafi']\",\n",
       " '[]',\n",
       " \"['amaqagency', 'scenes', 'islam', 'state', 'controlled', 'watch', 'download']\",\n",
       " \"['amaqagency', 'difficulties', 'faced', 'locals', 'travelling', 'aircraft', 'struck', 'main', 'bridge']\",\n",
       " \"['alamrikiomar', 'laykum', 'salaam', 'yes', 'every', 'create', 'shut', 'down', 'supporter']\",\n",
       " \"['israel', 'fighting', 'drones', 'knows', 'else']\",\n",
       " \"['watch']\",\n",
       " \"['realharampolice', 'ask', 'benjamin', 'netanyahu']\",\n",
       " \"['alamrikiomar', 'support', 'allamah', 'obama']\",\n",
       " \"['realharampolice', 'funny']\",\n",
       " \"['true']\",\n",
       " \"['exactly']\",\n",
       " \"['pay', 'attention', 'independent', 'freelance', 'journalist', 'important', 'tweeps', 'miss']\",\n",
       " \"['stopped', 'could', 'march', 'way', 'malikiyah', 'however', 'playing', 'long', 'game']\",\n",
       " \"['decades', 'stomach', 'long', 'wars', 'seeing', 'signs', 'already']\",\n",
       " \"['situation', 'anti', 'war', 'protests', 'america']\",\n",
       " \"['anti', 'war', 'protests', 'america', 'full', 'asking', 'hell', 'middle']\",\n",
       " \"['basically', 'kuffar', 'against', 'all', 'umamm', 'alkufr']\",\n",
       " \"['official', 'marines', 'expanding', 'combat', 'role', 'iraq', 'fox']\",\n",
       " \"['true']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['little', 'governments', 'federal', 'flemish', 'wallon', 'german', 'federation', 'brussels', 'wallonia']\",\n",
       " '[]',\n",
       " \"['allahuakbar']\",\n",
       " \"['sisters', 'follow', 'jazakom', 'mashallah', 'daily', 'inspiration', 'speaks', 'haq']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['belgium', 'forces', 'trying', 'break', 'through', 'bombing', 'suspects', 'apartment', 'hadath']\",\n",
       " \"['happening', 'saudi', 'arabia', 'shooting', 'inside', 'royal', 'palace', 'die', 'tonight', 'salool', 'sauod']\",\n",
       " \"['utunibsriqmltk2', 'hayyak', 'akhi']\",\n",
       " \"['admire', 'celebrity', 'doesn', 'exist', 'forget', 'made', 'exist']\",\n",
       " '[]',\n",
       " \"['bismillah', 'follow', 'jazakallahukhair', 'stay', 'away']\",\n",
       " \"['ameen']\",\n",
       " '[]',\n",
       " '[]',\n",
       " \"['radhiyallahu', 'anhu', 'wise', 'words']\",\n",
       " \"['shooting', 'inside', 'king', 'salman', 'alzheimer', 'palace', 'hell', 'salool', 'saud']\",\n",
       " \"['map', 'showing', 'hudna', 'truce', 'rebels', 'allowing', 'assad', 'free', 'resources', 'palmyra']\",\n",
       " \"['both', 'fake']\",\n",
       " \"['good', 'job']\",\n",
       " \"['hasbuna', 'alwakil', 'allah', 'protect']\",\n",
       " \"['video', 'add', 'watch']\",\n",
       " \"['against', 'small', 'muwahideen', 'akbar']\",\n",
       " '[]',\n",
       " \"['iraq', 'army', 'shelling', 'wounded', 'her', 'unborn', 'child']\",\n",
       " \"['vehicles', 'all', 'man', 'power', 'got', 'ambushed', 'bad', 'minutes', 'all', 'transferred', 'hell']\",\n",
       " \"['neutralized', 'usa', 'proxy', 'terrorists', 'pkk', 'nusaybin', 'town', 'mardin']\",\n",
       " \"['meet', 'jews', 'arabia', 'salool', 'saud']\",\n",
       " \"['times', 'understand', 'ppl', 'looking', 'wishing']\",\n",
       " \"['featured', 'fighter', 'praising', 'abdullah', 'baljiki', 'abu', 'mujahid', 'baljiki']\",\n",
       " \"['course', 'smile', 'murtad']\",\n",
       " \"['nusaybasmile', 'well', 'times', 'before', 'knows', 'dead', 'exactly', 'wants']\",\n",
       " \"['major', 'general', 'sha', 'aban', 'aloja', 'got', 'won', 'free', 'ticket', 'hell', 'paid']\",\n",
       " \"['british', 'fighter', 'survives', 'russia', 'airstrike', 'calls', 'pilot', 'prick']\",\n",
       " \"['assalamu', 'alaikum', 'suspension', 'follow', 'share']\",\n",
       " \"['goodness', 'sister', 'find', 'her', 'friends']\",\n",
       " \"['video', 'shows', 'bombing', 'milk', 'factory']\",\n",
       " \"['bomb', 'schools', 'universities', 'hospitals', 'markets', 'bridges', 'factories', 'infrastructure', 'wonder', 'target']\",\n",
       " \"['url']\",\n",
       " \"['regime', 'embodiment', 'manufacturer', 'exporter', 'terrorism']\",\n",
       " \"['americans', 'confirmed', 'walhamdulilah']\",\n",
       " \"['belgium', 'paris', 'suspect', 'abdeslam', 'refuses', 'talk', 'minister', 'afp']\",\n",
       " \"['apparently', 'death', '2nd', 'command', 'announced', 'actually', 'died', '2015']\",\n",
       " \"['biggest', 'murtad']\",\n",
       " \"['apr', 'rification', 'abdallah', 'baljiki', 'avait', 'menac', 'france', 'belgique', 'dans', 'une', 'vid', '2015']\",\n",
       " \"['operate']\",\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def join_tweets(tweet):\n",
    "    tweet_tokens_joined =[' '.join(words) for words in tweet]\n",
    "    return tweet_tokens_joined \n",
    "\n",
    "tweet_nostopwords = df_tweets['tweet_nostopwords'].tolist()\n",
    "tweet_tokens_joined=join_tweets(tweet_nostopwords)\n",
    "tweet_nostopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'militant' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-0e225cd10d33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_nostopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'militant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'militant' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# using word2vec model to find similar words\n",
    "#\n",
    "\n",
    "w2v_model = Word2Vec(min_count=5,\n",
    "                     window=4,\n",
    "                     size=100,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20)\n",
    "w2v_model.build_vocab(tweet_nostopwords, progress_per=10000)\n",
    "w2v_model.train(tweet_nostopwords, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.wv.most_similar(topn=10,positive='militant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8562129735946655, 'militant.'),\n",
       " (0.8422885537147522, '\"militant'),\n",
       " (0.7744318246841431, 'eyeonmilitant'),\n",
       " (0.7055030465126038, 'militant\"'),\n",
       " (0.6845437288284302, '155mm'),\n",
       " (0.6332287788391113, 'militants)'),\n",
       " (0.6322634220123291, 'yussuf'),\n",
       " (0.6221827268600464, 'kulgammumtaz'),\n",
       " (0.6207752823829651, 'ingushetia'),\n",
       " (0.6029382944107056, 'fatally')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using fasttext model to find similar words\n",
    "\n",
    "model = fasttext.train_unsupervised('tweets_clean.txt')\n",
    "model.get_nearest_neighbors('militant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Naive bayes classification model using nltk library\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=df_tweets[\"tweet_nostopwords\"]\n",
    "y=df_tweets[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=42, stratify=y)\n",
    "\n",
    "X_train = X_train.tolist() \n",
    "X_test = X_test.tolist() \n",
    "y_train = y_train.tolist() \n",
    "y_test = y_test.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_tweets(X,y):\n",
    "    tweets=[]\n",
    "    for tweet, label in zip( X,y):\n",
    "        tweets.append((tweet,label))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocabulary(train_data):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in train_data:\n",
    "        all_words.extend(words)\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:        \n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = buildVocabulary(processed_tweets(X_train, y_train))\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, processed_tweets(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( NBayesClassifier , open( \"tweet_pickled.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier = pickle.load(open( \"tweet_pickled.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in processed_tweets(X_test,y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7096496266513498\n",
      "F1 score: 0.5556043956043956\n",
      "Precision: 0.7069351230425056\n",
      "roc-auc score: 0.6664684345684153\n",
      "Recall 0.45763939174511226\n",
      "Confusion_matrix: \n",
      " [[1839  262]\n",
      " [ 749  632]]\n"
     ]
    }
   ],
   "source": [
    "#checking the model performance\n",
    "\n",
    "def model_performance():      \n",
    "    print(\"Accuracy:\",sklearn.metrics.accuracy_score(y_test, NBResultLabels))    \n",
    "    print(\"F1 score:\",sklearn.metrics.f1_score(y_test, NBResultLabels))\n",
    "    print(\"Precision:\",sklearn.metrics.precision_score(y_test, NBResultLabels))     \n",
    "    print(\"roc-auc score:\",sklearn.metrics.roc_auc_score(y_test, NBResultLabels))\n",
    "    print(\"Recall\",sklearn.metrics.recall_score(y_test, NBResultLabels))\n",
    "    print(\"Confusion_matrix: \\n\",sklearn.metrics.confusion_matrix(y_test, NBResultLabels))\n",
    "model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-08ec418e1139>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-08ec418e1139>\u001b[0m in \u001b[0;36mmake_xy\u001b[1;34m(tweet, vectorizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet_tokens_joined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    219\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "#Building a Naive bayes clssification model using scikit learn as nltk doesn't  support cross validation\n",
    "\n",
    "def make_xy(tweet, vectorizer=None):    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(tweet.tweet_tokens_joined)\n",
    "    X = X.tocsc()  \n",
    "    y = tweet.label.values.astype(np.int)\n",
    "    return X, y\n",
    "X, y = make_xy(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [nan nan nan nan nan]\n",
      "F1 score: [nan nan nan nan nan]\n",
      "roc-auc score: [nan nan nan nan nan]\n",
      "precision: [nan nan nan nan nan]\n",
      "recall: [nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['assalamu', 'alaikum', 'follow']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['assalamu', 'alaikum', 'follow']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['assalamu', 'alaikum', 'follow']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['assalamu', 'alaikum', 'follow']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['assalamu', 'alaikum', 'follow']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-4b399ee1e8fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mconf_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Confusion Matrix:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel_performance_crossval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-4b399ee1e8fe>\u001b[0m in \u001b[0;36mmodel_performance_crossval\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"precision:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"recall:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'recall'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mconf_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Confusion Matrix:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    753\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[0;32m    754\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[1;32m--> 755\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[1;31m# Concatenate the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \"\"\"\n\u001b[1;32m--> 609\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    756\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    752\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    753\u001b[0m         \"\"\"\n\u001b[1;32m--> 754\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"['bismillah', 'acc', 'number', 'follow', 'jzk']\""
     ]
    }
   ],
   "source": [
    "#Model performnce after cross validation\n",
    "\n",
    "def model_performance_crossval():\n",
    "    clf = MultinomialNB(alpha=.1)\n",
    "    k_fold = KFold( n_splits=5, shuffle=True, random_state=0)\n",
    "    print(\"Accuracy:\", cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='accuracy'))\n",
    "    print(\"F1 score:\",cross_val_score(clf, X, y, n_jobs=1, cv=k_fold,scoring='f1'))\n",
    "    print(\"roc-auc score:\",cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='roc_auc'))\n",
    "    print(\"precision:\",cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='precision'))\n",
    "    print(\"recall:\",cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='recall'))\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=k_fold)\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "model_performance_crossval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing empty rows and rows with a single word \n",
    "def emptyrows_remove(tweet, nostopwords):    \n",
    "    min_words = 1\n",
    "    tweet = tweet[nostopwords.apply(lambda x: True  if len(x)>min_words else False)]      \n",
    "    return tweet\n",
    "\n",
    "df_tweets = emptyrows_remove(df_tweets, df_tweets['tweet_nostopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-8e1640106840>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Checking performance after removing empty rows and rows with a single word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel_performance_crossval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-08ec418e1139>\u001b[0m in \u001b[0;36mmake_xy\u001b[1;34m(tweet, vectorizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet_tokens_joined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IDE\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    219\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Checking performance after removing empty rows and rows with a single word \n",
    "\n",
    "X, y = make_xy(df_tweets)\n",
    "model_performance_crossval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(y,columns=['Output'])\n",
    "df.to_excel(excel_writer = \"C:\\\\Users\\\\USER\\\\Documents\\\\Geetha\\\\Data Science\\\\Github\\\\Springboard\\\\Capstone Poject 1\\\\test.xlsx\",encoding=\"ISO-8859-1\")\n",
    "df_tweets['Prediction'] =df['Output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
